{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional  Attention Long Sort Term Memory Network\n",
    "This notebook train a model (for validate) with 20 classes from kinetics..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 1024\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "units = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_PATH_TRAIN = '/media/DATA/kinetics/features/train'\n",
    "FEATURES_PATH_VALIDATION = '/media/DATA/kinetics/features/validation'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get how many classes have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of 20 classes in this dataset.\n"
     ]
    }
   ],
   "source": [
    "class_count = len(os.listdir(FEATURES_PATH_TRAIN))\n",
    "\n",
    "print(f'Total of {class_count} classes in this dataset.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get classes and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classes_by_features(base_path):\n",
    "    classes = []\n",
    "    features = []\n",
    "    \n",
    "    for clazz in os.listdir(base_path):\n",
    "        \n",
    "        features_path = os.path.join(base_path, clazz)\n",
    "        for feature in os.listdir(features_path):\n",
    "            classes.append(clazz)\n",
    "            features.append(feature)\n",
    "    \n",
    "    return classes, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classes, train_features = get_classes_by_features(FEATURES_PATH_TRAIN)\n",
    "validation_classes, validation_features = get_classes_by_features(FEATURES_PATH_VALIDATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classes_to_labels(items):\n",
    "    _labels = {}\n",
    "    _classes = {}\n",
    "    for label in set(items):\n",
    "        _labels[label] = len(_labels)\n",
    "        _classes[_labels[label]] = label\n",
    "    return _labels, _classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, classes = classes_to_labels(train_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "_validation_classes = []\n",
    "_validation_features = []\n",
    "for clazz, feature in zip(validation_classes, validation_features):\n",
    "    if clazz in train_classes:\n",
    "        _validation_classes.append(clazz)\n",
    "        _validation_features.append(feature)\n",
    "validation_classes = _validation_classes\n",
    "validation_features = _validation_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = [tf.one_hot(labels[clazz], len(labels)) for clazz in train_classes]\n",
    "validation_labels = [tf.one_hot(labels[clazz], len(labels)) for clazz in validation_classes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_classes, train_labels, train_features))\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((validation_classes, validation_labels, validation_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods to load `npy` files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.autograph.experimental.do_not_convert\n",
    "def get_feature_tensor(features_base_path, clazz, label, feature):\n",
    "    npy_feature = np.load(os.path.join(features_base_path, clazz, feature))\n",
    "    if npy_feature.shape[0] < 200:\n",
    "        raise Exception(f'Ignored... {feature}')\n",
    "    return tf.reshape(npy_feature[:200], (200, -1, npy_feature.shape[-1])), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_numpy_features_lambda(feature_path):\n",
    "    return lambda clazz, label, feature: \\\n",
    "        tf.numpy_function(get_feature_tensor, [feature_path, clazz, label, feature], [tf.float64, tf.float32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure datasets with hyperparam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_dataset = train_dataset \\\n",
    "                .shuffle(BUFFER_SIZE) \\\n",
    "                .map(map_numpy_features_lambda(FEATURES_PATH_TRAIN), \\\n",
    "                    num_parallel_calls=tf.data.experimental.AUTOTUNE) \\\n",
    "                .apply(tf.data.experimental.ignore_errors()) \\\n",
    "                .batch(BATCH_SIZE) \\\n",
    "                .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset = validation_dataset \\\n",
    "                .shuffle(BUFFER_SIZE) \\\n",
    "                .map(map_numpy_features_lambda(FEATURES_PATH_VALIDATION), \\\n",
    "                     num_parallel_calls=tf.data.experimental.AUTOTUNE) \\\n",
    "                .apply(tf.data.experimental.ignore_errors()) \\\n",
    "                .batch(BATCH_SIZE) \\\n",
    "                .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features):\n",
    "        attention_hidden_layer = tf.nn.tanh(self.W1(features))\n",
    "\n",
    "        score = self.V(attention_hidden_layer)\n",
    "\n",
    "        attention_weights = tf.nn.softmax(score, axis=-1)\n",
    "\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv = tf.keras.layers.Conv2D(filters=1048, \\\n",
    "                                           kernel_size=(1, 2), \\\n",
    "                                           strides=(1, 2),  \\\n",
    "                                           bias_initializer='glorot_uniform')\n",
    "        \n",
    "        self.conv_2 = tf.keras.layers.Conv2D(filters=512, \\\n",
    "                                             kernel_size=(1, 2),  \\\n",
    "                                             strides=(1, 2),  \\\n",
    "                                             bias_initializer='glorot_uniform')\n",
    "                                             \n",
    "        \n",
    "        self.conv_3 = tf.keras.layers.Conv2D(filters=256,  \\\n",
    "                                             kernel_size=(1, 2),  \\\n",
    "                                             strides=(1, 2),  \\\n",
    "                                             bias_initializer='glorot_uniform')\n",
    "\n",
    "\n",
    "    def call(self, x):\n",
    "        # x has shape (batch, frames, 64, 2048)\n",
    "        # conv output shape (batch, frames, 32, 1048)\n",
    "        x = self.conv(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        # conv_2 output shape (batch, frames, 16, 512)\n",
    "        x = self.conv_2(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        # conv_3 output shape (batch, frames, 8, 256)\n",
    "        x = self.conv_3(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        # reshape output shape (batch, frames, 2048)\n",
    "        x = tf.reshape(x, (x.shape[0], x.shape[1], -1))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, units, classes_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.units = units\n",
    "        \n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "        \n",
    "        self.lstm = tf.keras.layers.LSTM(self.units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        \n",
    "        self.fc1 = tf.keras.layers.Dense(classes_size)\n",
    "        \n",
    "        self.global_avg = tf.keras.layers.GlobalAveragePooling1D()\n",
    "        \n",
    "        self.fc2 = tf.keras.layers.Dense(classes_size)\n",
    "        \n",
    "        self.dropout = tf.keras.layers.Dropout(0.2)\n",
    "\n",
    "    def call(self, features):\n",
    "        # context vector shape (batch, 1024)\n",
    "        context_vector, attention_weights = self.attention(features)\n",
    "        \n",
    "        # x (features with attention) shape (batch, frames, 1024)\n",
    "        x = features * tf.expand_dims(context_vector, 1)\n",
    "        x = tf.nn.tanh(x)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # RNN (LSTM) output shape (batch, frames, 512)\n",
    "        output, state, _ = self.lstm(x)\n",
    "        x = tf.nn.relu(output)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # x shape (batch, frames, classes_size)\n",
    "        x = self.fc1(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # global average shape (batch, classes_size)\n",
    "        x = self.global_avg(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # x shape (batch, classes_size)\n",
    "        x = self.fc2(x)\n",
    "        x = tf.nn.sigmoid(x)\n",
    "\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder()\n",
    "decoder = Decoder(units, class_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(tensors):\n",
    "    features = encoder(tensors)\n",
    "    predictions, state, attentions = decoder(features)\n",
    "    return predictions, state, attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD()\n",
    "accuraccy_metric = tf.keras.metrics.CategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    return tf.reduce_mean(tf.keras.losses.categorical_crossentropy(real, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer = optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(tensors, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _, _ = evaluate(tensors)\n",
    "        loss = loss_function(targets, predictions)\n",
    "\n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    \n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "    \n",
    "    return loss, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_step():\n",
    "    start = time.time()\n",
    "    losses = []\n",
    "    for (batch, (tensors, targets)) in enumerate(validation_dataset):\n",
    "        accuraccy_metric.reset_states()\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions, _, _ = evaluate(tensors)\n",
    "            losses.append(loss_function(targets, predictions))\n",
    "            accuraccy_metric.update_state(targets, predictions)\n",
    "    \n",
    "    print('Validation loss {:.4f}, acc: {:.2f}% and time taken {:.2f} sec' \\\n",
    "          .format(tf.reduce_mean(loss).numpy(), accuraccy_metric.result() * 100, time.time() - start))\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load weights from checkpoint manager and execute train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "    # restoring the latest checkpoint in checkpoint_path\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 0, Loss 2.9919, Acc: 0.00%\n",
      "Epoch 2, Batch 50, Loss 2.9936, Acc: 0.00%\n",
      "Epoch 2, Batch 100, Loss 2.9942, Acc: 16.67%\n",
      "Epoch 2 with loss 2.9942\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[6,2048,200,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Conv2D]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-e84b8cd4bf8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch {} with loss {:.4f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Time taken for an epoch {:.2f} sec! \\n\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-118-ab403386f2c8>\u001b[0m in \u001b[0;36mvalidation_step\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0maccuraccy_metric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0maccuraccy_metric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-113-61ca6c62aafe>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattentions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattentions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/conv-alstm-nn/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1011\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1012\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-110-f9315fdbf9bc>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# x has shape (batch, frames, 64, 2048)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# conv output shape (batch, frames, 32, 1048)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/conv-alstm-nn/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1011\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1012\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/conv-alstm-nn/lib/python3.8/site-packages/tensorflow/python/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_causal_padding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convolution_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/conv-alstm-nn/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/conv-alstm-nn/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconvolution_v2\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m     name=None):\n\u001b[0;32m-> 1013\u001b[0;31m   return convolution_internal(\n\u001b[0m\u001b[1;32m   1014\u001b[0m       \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=redefined-builtin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m       \u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/conv-alstm-nn/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconvolution_internal\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name, call_from_convolution, num_spatial_dims)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv1d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1143\u001b[0;31m       return op(\n\u001b[0m\u001b[1;32m   1144\u001b[0m           \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m           \u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/conv-alstm-nn/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m_conv2d_expanded_batch\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   2595\u001b[0m     \u001b[0;31m# We avoid calling squeeze_batch_dims to reduce extra python function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2596\u001b[0m     \u001b[0;31m# call slowdown in eager mode.  This branch doesn't require reshapes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2597\u001b[0;31m     return gen_nn_ops.conv2d(\n\u001b[0m\u001b[1;32m   2598\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2599\u001b[0m         \u001b[0mfilter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/conv-alstm-nn/lib/python3.8/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m    934\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 936\u001b[0;31m       return conv2d_eager_fallback(\n\u001b[0m\u001b[1;32m    937\u001b[0m           \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m           \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/conv-alstm-nn/lib/python3.8/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d_eager_fallback\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name, ctx)\u001b[0m\n\u001b[1;32m   1023\u001b[0m   \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"padding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"explicit_paddings\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m   explicit_paddings, \"data_format\", data_format, \"dilations\", dilations)\n\u001b[0;32m-> 1025\u001b[0;31m   _result = _execute.execute(b\"Conv2D\", 1, inputs=_inputs_flat, attrs=_attrs,\n\u001b[0m\u001b[1;32m   1026\u001b[0m                              ctx=ctx, name=name)\n\u001b[1;32m   1027\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/conv-alstm-nn/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[6,2048,200,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Conv2D]"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "losses = []\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "    loss_epoch = []\n",
    "    \n",
    "    for (batch, (tensors, targets)) in enumerate(train_dataset):\n",
    "        accuraccy_metric.reset_states()\n",
    "        \n",
    "        loss, predictions = train_step(tensors, targets)\n",
    "        \n",
    "        accuraccy_metric.update_state(targets, predictions)\n",
    "        losses.append(loss)\n",
    "        loss_epoch.append(loss)\n",
    "        \n",
    "        if batch % 50 == 0: \n",
    "            print('Epoch {}, Batch {}, Loss {:.4f}, Acc: {:.2f}%' \\\n",
    "                  .format(epoch + 1, batch, loss.numpy(), accuraccy_metric.result().numpy() * 100))\n",
    "    \n",
    "    print('Epoch {} with loss {:.4f}'.format(epoch + 1, tf.reduce_mean(loss_epoch).numpy()))\n",
    "\n",
    "    _ = validation_step()\n",
    "    \n",
    "    print('Time taken for an epoch {:.2f} sec! \\n\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save weights of model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./checkpoints/train/ckpt-1'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_manager.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot loss by batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)\n",
    "plt.xlabel('Batchs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot by batch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute train validate with train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction class: [1] and correct class: [9]\n",
      "Prediction value [0.5187836]\n",
      "Loss: 2.9361214637756348 and acc: 0.0%\n"
     ]
    }
   ],
   "source": [
    "index = 1\n",
    "_train_class = train_classes[index]\n",
    "_train_label = train_labels[index]\n",
    "_train_feature = train_features[index]\n",
    "_train_feature, _ = get_feature_tensor(FEATURES_PATH_TRAIN, _train_class, _train_label, _train_feature)\n",
    "_train_feature = tf.expand_dims(_train_feature, axis=0)\n",
    "_train_label = tf.expand_dims(_train_label, axis=0)\n",
    "\n",
    "loss, predictions = train_step(_train_feature, _train_label)\n",
    "accuraccy_metric.reset_states()\n",
    "accuraccy_metric.update_state(_train_label, predictions)\n",
    "print(f'Prediction class: {tf.math.argmax(predictions, axis=-1)} \\\n",
    "and correct class: {tf.math.argmax(_train_label, axis=-1)}')\n",
    "print(f'Prediction value {tf.reduce_max(predictions, axis=-1)}')\n",
    "print(f'Loss: {loss} and acc: {accuraccy_metric.result() * 100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
